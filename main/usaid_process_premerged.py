"""
Script for processing premerged data generated by usaid_balanced_premerge.py.

MBKM: With 2GB of data, it is possible to fit a MBKM in one batch just using the 'fit' method.

Birch: With partial_fit, birch takes ~90s for 1M pixels, 250 clusters. Unlike MBKM, Birch doesn't speed up after the first
  iteration. In fact, birch's memory use increases with each iteration. We can't get through all the data without crashing.
  -If I reduce the number of clusters to 60, memory usage reduces, but we still eventually get a memory error.
  -As per sklearn's user guide, setting n_clusters=None eliminates the big memory spikes. 1M pixel batches take ~110s
    each, so the entire set should take ~82 minutes. Once we've added every batch, we can then set n_clusters and make a
    final call to partial_fit. Unfortunately, the final call to parital_fit causes a memoryerror crash.
  -Increasing the threshold param will decrease the number of subclusters formed before we set n_clusters. This reduces
    memory usage during the final fit.
  -The partial fit time seems to depend on the current number of subclusters. So, increasing tolerance makes the process
    a little faster.


Spectral: this algorithm doesnt have a partial_fit method. Without it we get a MemoryError crash. Is it possible to use
  it on large data sets?
"""
import pandas as pd
from satellite_head import *
from timeit import default_timer as timer
from sklearn.preprocessing import StandardScaler
from sklearn import cluster
import pickle
from sklearn.neighbors import kneighbors_graph


#Hardcoded spectral band names:
BAND_LIST = ['B01','B02','B03','B04','B05','B06','B07','B08','B8A','B09','B10','B11','B12']
BAND_REZS = [60,   10,    10,    10,   20,   20,   20,   10,   20,   60,   60,   20,   20]
#Harcoded path to pickled StandardScalers dir:
SS_DIR = './USAID_africa/StandardScalers/'

#Hardcoded file path for dataframe:
USAID_DATA_PATH = './USAID_africa/premerge_data_rbt5.csv'
#Hardcoded file path for big npy pixel matrix:
PXL_NPY_PATH = './USAID_africa/premerge_data_rbt5.npy'
#Hardcoded balance tolerance factor for re-balancing:
BALANCE_TOL = 5
#Hardcoded batch size for partial_fit:
BSIZE = 1000000
#Hardcoded dir for output files:
OUT_DIR = './USAID_africa/premerged/'

def balance_classes(df, btol):
    """Even up the support of the classes. Original index is preserved so that we can use it to select parts of pxl_mat."""
    #Find the least supported class and muliply by the tolerance coefficient to get max_count:
    ccounts = df['classification'].value_counts()
    max_count = np.min(ccounts.values) * btol
    #Create a new dataframe with balenced support:
    newdf = pd.DataFrame(columns=df.columns.values)
    for x in df.groupby('classification'):
        if x[1].shape[0] > max_count:
            newdf = newdf.append(x[1].sample(max_count))
        else:
            newdf = newdf.append(x[1])
    return newdf

t0 = timer()
###Load datafiles:
master_df = pd.read_csv(USAID_DATA_PATH)
print('Loading pxl_mat from .npy file...')
t1 = timer()
pxl_mat = np.load(PXL_NPY_PATH)
print('Loading took {:.1f}s'.format(timer()-t1))

# ###Re-balance data:
# OUT_PATH = './USAID_africa/premerge_data_rbt5'
# master_df = balance_classes(master_df, BALANCE_TOL)
# #Apply re-balanced indicies to pxl_mat:
# pxl_mat = pxl_mat[master_df.index.values,:,:]
# #Save the new subset:
# print('Saving re-balanced data...')
# master_df.to_csv(OUT_PATH+'.csv',index=False)
# np.save(OUT_PATH+'.npy', pxl_mat)

# ###Randomly plot 10 of the view areas:
# for i in np.random.choice(master_df.shape[0], 10, replace=False):
#     red = pxl_mat[i,:,3].reshape(200,200)
#     green = pxl_mat[i,:,2].reshape(200,200)
#     blue = pxl_mat[i,:,1].reshape(200,200)
#     imshow_to_file(scale_down(np.stack((red, green, blue), axis=2),13.8),
#                    'random_crop_{}.png'.format(i),
#                    title=master_df.loc[i,'classification'])

###Train and predict cluster models:
#Reshape:
pxl_mat = pxl_mat.reshape(-1,13)
print('Training cluster models on {:.1f} M pixels ({:.1f}% of a full ESA image).'.format(pxl_mat.shape[0]/10**6, pxl_mat.shape[0]*100/10980**2))
#Apply StandardScaler:
t1 = timer()
for b,band in enumerate(BAND_LIST):
    with open(SS_DIR+band+'_grand_SS.pkl','rb') as pfile:
        ss = pickle.load(pfile)
    pxl_mat[:,b] = ss.transform(pxl_mat[:,b].reshape(-1, 1)).reshape(-1)
print('StandardScaler took {:.1f}s'.format(timer()-t1))

#Set up multiple algorithms:
print("Initializing clustering algorithms...")
# # connectivity matrix for structured Ward
# connectivity = kneighbors_graph(pxl_mat, n_neighbors=10, include_self=False)
# # make connectivity symmetric
# connectivity = 0.5 * (connectivity + connectivity.T)

# alg = cluster.SpectralClustering(n_clusters=250,eigen_solver='arpack',affinity="nearest_neighbors")

# alg_list = [cluster.MiniBatchKMeans(n_clusters=60,compute_labels=False),
#             cluster.MiniBatchKMeans(n_clusters=250,compute_labels=False),
#             cluster.MiniBatchKMeans(n_clusters=500,compute_labels=False),
#             cluster.MiniBatchKMeans(n_clusters=1000,compute_labels=False),
#             # cluster.AgglomerativeClustering(n_clusters=250, linkage='ward',connectivity=connectivity),
#             cluster.Birch(n_clusters=250,compute_labels=False)]
#
# for p in range(0,pxl_mat.shape[0],BSIZE):
#     print('Using parital_fit with {:.2f}% of pixels already processed...'.format(p*100/pxl_mat.shape[0]))
#     for a in range(len(alg_list)):
#         t2 = timer()
#         alg_name = alg_list[a].__class__.__name__
#         alg_list[a].partial_fit(pxl_mat[p:p+BSIZE,:])
#         n_clusters = alg_list[a].n_clusters
#         print("{} cluster {}: {:.1f}s".format(n_clusters, alg_name, timer()-t2))
#
# for alg in alg_list:
#     #Save algorithm:
#     filename = OUT_DIR + alg.__class__.__name__ + '_n{}_rbt5.pkl'.format(n_clusters)
#     with open(filename,'wb') as pfile:
#         pickle.dump(alg,pfile,pickle.HIGHEST_PROTOCOL)

###Fit MBKMs using full fits:
alg_list = [cluster.MiniBatchKMeans(n_clusters=30,compute_labels=False),
            cluster.MiniBatchKMeans(n_clusters=120,compute_labels=False),
            cluster.MiniBatchKMeans(n_clusters=2000,compute_labels=False),
            cluster.MiniBatchKMeans(n_clusters=3000,compute_labels=False),
            cluster.MiniBatchKMeans(n_clusters=4000,compute_labels=False)]
for alg in alg_list:
    t2 = timer()
    alg_name = alg.__class__.__name__
    n_clusters = alg.n_clusters
    alg.fit(pxl_mat)
    print("{} cluster {}: {:.1f}s".format(n_clusters,alg_name,timer()-t2))
    #Save algorithm:
    filename = OUT_DIR + alg_name + '_n{}_rbt5.pkl'.format(n_clusters)
    with open(filename,'wb') as pfile:
        pickle.dump(alg,pfile,pickle.HIGHEST_PROTOCOL)



print('Total time: {:.2f} hours'.format((timer()-t0)/3600))
myalert()
print('Done')

# ###Predict clusters in one view area:
# ypred = np.zeros(200*200,dtype='int32')
# for p in range(0,200*200,1000):
#     print('On pixel {}'.format(p))
#     ypred[p:p+1000] = brc.predict(pxl_mat[p:p+1000,:])
# #Plot cluster labels:
# print('Plotting clusters...')
# n_clusters = np.max(ypred)+1
# np.random.seed(285)
# colors = np.random.rand(n_clusters,3).astype('float32')
# c = colors[ypred]
# imshow_to_file(c.reshape((200,200,3)),
#                'birch_premerged_clustering.png',
#                title='rbt5 tr1 birch model ({} clusters)'.format(n_clusters))
